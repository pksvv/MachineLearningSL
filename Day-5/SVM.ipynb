{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines with Scikit-learn\n",
    "### One of the most popular and widely used supervised machine learning algorithms.\n",
    "\n",
    "* SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. \n",
    "\n",
    "\n",
    "* It is known for its kernel trick to handle nonlinear input spaces. \n",
    "\n",
    "\n",
    "* It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition.\n",
    "\n",
    "##### SVM is an exciting algorithm and the concepts are relatively simple. \n",
    "\n",
    "##### The classifier separates data points using a hyperplane with the largest amount of margin. \n",
    "\n",
    "##### That's why an SVM classifier is also known as a discriminative classifier. \n",
    "\n",
    "##### SVM finds an optimal hyperplane which helps in classifying new data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/svm1.JPG'>\n",
    "\n",
    "## Generally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. \n",
    "\n",
    "* It can easily handle multiple continuous and categorical variables. \n",
    "\n",
    "\n",
    "* SVM constructs a hyperplane in multidimensional space to separate different classes. \n",
    "\n",
    "\n",
    "* SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. \n",
    "\n",
    "\n",
    "* The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors\n",
    "* Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n",
    "\n",
    "### Hyperplane\n",
    "* A hyperplane is a decision plane which separates between a set of objects having different class memberships.\n",
    "\n",
    "### Margin\n",
    "* A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does SVM work?\n",
    "\n",
    "#### The main objective is to segregate the given dataset in the best possible way. \n",
    "\n",
    "* The distance between the either nearest points is known as the margin. \n",
    "* The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. \n",
    "\n",
    "### SVM searches for the maximum marginal hyperplane in the following steps:\n",
    "\n",
    "1. Generate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n",
    "\n",
    "\n",
    "\n",
    "2. Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n",
    "\n",
    "<img src='img/svm2.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with non-linear and inseparable planes\n",
    "\n",
    "\n",
    "Some problems canâ€™t be solved using linear hyperplane, as shown in the figure below (left-hand side).\n",
    "\n",
    "<br><br>\n",
    "<img src='img/svm3.JPG'>\n",
    "* In such situation, SVM uses a **kernel trick** to transform the input space to a higher dimensional space as shown on the right. \n",
    "\n",
    "\n",
    "* The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x^2=y^2). \n",
    "\n",
    "\n",
    "* Now you can easily segregate these points using linear separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Kernels\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel. \n",
    "\n",
    "\n",
    "A kernel transforms an input data space into the required form. \n",
    "\n",
    "### Kernel Trick\n",
    "* SVM uses a technique called the kernel trick. \n",
    "    * Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. \n",
    "\n",
    "* In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. \n",
    "\n",
    "* It is most useful in non-linear separation problem. \n",
    "\n",
    "* Kernel trick helps you to build a more accurate classifier.\n",
    "\n",
    "### Types of Kernels\n",
    "<br>\n",
    "\n",
    "* Linear Kernel \n",
    "    * A linear kernel can be used as normal dot product any two given observations. \n",
    "    * The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* Polynomial Kernel \n",
    "    * A polynomial kernel is a more generalized form of the linear kernel. \n",
    "    * The polynomial kernel can distinguish curved or nonlinear input space.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* Radial Basis Function Kernel \n",
    "    * The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. \n",
    "    * RBF can map an input space in infinite dimensional space.\n",
    "\n",
    "# SVM Classifier Building in Scikit-learn\n",
    "\n",
    "\n",
    "In the model the building part, you can use the cancer dataset, which is a very famous multi-class classification problem. \n",
    "\n",
    "This dataset is computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "The dataset comprises 30 features (mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, and worst fractal dimension) and a target (type of cancer).\n",
    "\n",
    "This data has two types of cancer classes: malignant (harmful) and benign (not harmful). Here, you can build a model to classify the type of cancer. \n",
    "\n",
    "The dataset is available in the scikit-learn library or you can also download it from the UCI Machine Learning Library.\n",
    "\n",
    "## Loading Data\n",
    "Let's first load the required dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "After you have loaded the dataset, you might want to know a little bit more about it. You can check feature and target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the 13 features\n",
    "print(\"Features: \", cancer.feature_names)\n",
    "\n",
    "# print the label type of cancer('malignant' 'benign')\n",
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore it for a bit more. You can also check the shape of the dataset using shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking top 5 records of the feature set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]]\n"
     ]
    }
   ],
   "source": [
    "# print the cancer data features (top 5 records)\n",
    "print(cancer.data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Model\n",
    "Let's build support vector machine model. \n",
    "First, import the SVM module and create support vector classifier object by passing argument kernel as the linear kernel in SVC() function.\n",
    "\n",
    "Then, fit your model on train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIPUL.GAUR\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='poly') # Linear Kernel\n",
    "#kernel='linear'\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "Let's estimate how accurately the classifier or model can predict the breast cancer of patients.\n",
    "\n",
    "Accuracy can be computed by comparing actual test set values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got a classification rate of 96.49%, considered as very good accuracy.\n",
    "\n",
    "For further evaluation, you can also check precision and recall of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9811320754716981\n",
      "Recall: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Got a precision of 98% and recall of 96%, which are considered as very good values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
