{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing Words Using WordNet\n",
    "* Part-of-speech constants:\n",
    "* ADJ: a\n",
    "* ADV: r\n",
    "* NOUN: n\n",
    "* VERB: v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definit\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "print(stemmer.stem('definitions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('definitions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing words by specifying parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective:  running\n",
      "Adverb:  running\n",
      "Noun:  running\n",
      "Verb:  run\n"
     ]
    }
   ],
   "source": [
    "print('Adjective: ', wnl.lemmatize('running', pos='a'))\n",
    "print('Adverb: ', wnl.lemmatize('running', pos='r'))\n",
    "print('Noun: ', wnl.lemmatize('running', pos='n'))\n",
    "print('Verb: ', wnl.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = ['dictionaries', 'dictionary', \n",
    "                'hushed', 'hush', 'hushing',\n",
    "                'functional', 'functionally',\n",
    "                'lying', 'lied', 'lies',\n",
    "                'flawed', 'flaws', 'flawless', \n",
    "                'friendship', 'friendships', 'friendly', 'friendless', \n",
    "                'definitions', 'definition', 'definitely',  \n",
    "                'the', 'these', 'those',\n",
    "                'motivational', 'motivate', 'motivating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english')\n",
    "\n",
    "ss_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ss_stemmed_tokens.append(ss.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl_lemmatized_tokens = []\n",
    "for token in input_tokens:\n",
    "    wnl_lemmatized_tokens.append(wnl.lemmatize(token, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <th>WordNet Lemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dictionaries</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hushing</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>functionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lies</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flawed</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flaws</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>definitions</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>definition</td>\n",
       "      <td>definit</td>\n",
       "      <td>definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definitely</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words Snowball Stemmer WordNet Lemmatizer\n",
       "0   dictionaries       dictionari       dictionaries\n",
       "1     dictionary       dictionari         dictionary\n",
       "2         hushed             hush               hush\n",
       "3           hush             hush               hush\n",
       "4        hushing             hush               hush\n",
       "5     functional         function         functional\n",
       "6   functionally         function       functionally\n",
       "7          lying              lie                lie\n",
       "8           lied              lie                lie\n",
       "9           lies              lie                lie\n",
       "10        flawed             flaw               flaw\n",
       "11         flaws             flaw               flaw\n",
       "12      flawless         flawless           flawless\n",
       "13    friendship       friendship         friendship\n",
       "14   friendships       friendship        friendships\n",
       "15      friendly           friend           friendly\n",
       "16    friendless       friendless         friendless\n",
       "17   definitions          definit        definitions\n",
       "18    definition          definit         definition\n",
       "19    definitely          definit         definitely\n",
       "20           the              the                the\n",
       "21         these            these              these\n",
       "22         those            those              those\n",
       "23  motivational            motiv       motivational\n",
       "24      motivate            motiv           motivate\n",
       "25    motivating            motiv           motivate"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_lemmas_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens,\n",
    "    'WordNet Lemmatizer': wnl_lemmatized_tokens\n",
    "})\n",
    "\n",
    "stems_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.\n",
      "Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.\n",
      "Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.\n",
      "In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.\n",
      "They were married in 1895.\n",
      "The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.\n",
      "In July 1898, the Curies announced the discovery of a new chemical element, polonium.\n",
      "At the end of the year, they announced the discovery of another, radium.\n",
      "The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.\n",
      "Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\n",
      "Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.\n",
      "She received a second Nobel Prize, for Chemistry, in 1911.\n",
      "The Curie's research was crucial in the development of x-rays in surgery.\n",
      "During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.\n",
      "The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.\n",
      "Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.\n",
      "By the late 1920s her health was beginning to deteriorate.\n",
      "She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.\n",
      "The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "with open('./datasets/biography.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "    \n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    lemmatized_words.append(wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Marie Curie be a Polish-born physicist and chemist and one of the most famous scientists of her time . Together with her husband Pierre , she be award the Nobel Prize in 1903 , and she go on to win another in 1911 . Marie Sklodowska be bear in Warsaw on 7 November 1867 , the daughter of a teacher . In 1891 , she go to Paris to study physics and mathematics at the Sorbonne where she meet Pierre Curie , professor of the School of Physics . They be marry in 1895 . The Curies work together investigate radioactivity , build on the work of the German physicist Roentgen and the French physicist Becquerel . In July 1898 , the Curies announce the discovery of a new chemical element , polonium . At the end of the year , they announce the discovery of another , radium . The Curies , along with Becquerel , be award the Nobel Prize for Physics in 1903 . Pierre 's life be cut short in 1906 when he be knock down and kill by a carriage . Marie take over his teach post , become the first woman to teach at the Sorbonne , and devote herself to continue the work that they have begin together . She receive a second Nobel Prize , for Chemistry , in 1911 . The Curie 's research be crucial in the development of x-ray in surgery . During World War One Curie help to equip ambulances with x-ray equipment , which she herself drive to the front line . The International Red Cross make her head of its radiological service and she hold train course for medical orderlies and doctor in the new techniques . Despite her success , Marie continue to face great opposition from male scientists in France , and she never receive significant financial benefit from her work . By the late 1920s her health be begin to deteriorate . She die on 4 July 1934 from leukaemia , cause by exposure to high-energy radiation from her research . The Curies ' eldest daughter Irene be herself a scientist and winner of the Nobel Prize for Chemistry .\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
