{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In machine learning, two tasks are commonly done at the same time in data pipelines: \n",
    "\n",
    "# 1. Cross validation \n",
    "\n",
    "# 2. (hyper)parameter tuning. \n",
    "\n",
    "#### Cross validation is the process of training learners using one set of data and testing it using a different set. \n",
    "\n",
    "#### Parameter tuning is the process to selecting the values for a model’s parameters that maximize the accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets, svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Two Datasets\n",
    "\n",
    "\n",
    "In the code below, we load the digits dataset, which contains 64 feature variables. \n",
    "\n",
    "\n",
    "Each feature denotes the darkness of a pixel in an 8 by 8 image of a handwritten digit. We can see these features for the first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the digit data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# View the features of the first observation\n",
    "digits.data[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The target data is a vector containing the image’s true digit. For example, the first observation is a handwritten digit for ‘0’.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the target of the first observation\n",
    "digits.target[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To demonstrate cross validation and parameter tuning, first we are going to divide the digit data into two datasets called data1 and data2. \n",
    "\n",
    "\n",
    "* data1 contains the first 1000 rows of the digits data, while data2 contains the remaining ~800 rows. \n",
    "\n",
    "\n",
    "* Note that this split is separate to the cross validation we will conduct and is done purely to demonstrate something at the end of the tutorial. \n",
    "\n",
    "\n",
    "* In other words, don’t worry about data2 for now, we will come back to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset 1\n",
    "data1_features = digits.data[:1000]\n",
    "data1_target = digits.target[:1000]\n",
    "\n",
    "# Create dataset 2\n",
    "data2_features = digits.data[1000:]\n",
    "data2_target = digits.target[1000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Parameter Candidates\n",
    "\n",
    "\n",
    "Before looking for which combination of parameter values produces the most accurate model, we must specify the different candidate values we want to try. \n",
    "\n",
    "\n",
    "In the code below we have a number of candidate parameter values, including four different values for C (1, 10, 100, 1000), two values for gamma (0.001, 0.0001), and two kernels (linear, rbf). \n",
    "\n",
    "\n",
    "The grid search will try all combinations of parameter values and select the set of parameters which provides the most accurate model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Grid Search To Find Parameters Producing Highest Score\n",
    "\n",
    "\n",
    "Now we are ready to conduct the grid search using scikit-learn’s GridSearchCV which stands for grid search cross validation. \n",
    "\n",
    "\n",
    "By default, the GridSearchCV’s cross validation uses 3-fold KFold or StratifiedKFold depending on the situation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIPUL.GAUR\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(data1_features, data1_target)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We have our results! First, let’s look at the accuracy score when we apply the model to the data1’s test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which parameters are the best? We can tell scikit-learn to display them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Best Kernel: rbf\n",
      "Best Gamma: 0.001\n"
     ]
    }
   ],
   "source": [
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',clf.best_estimator_.C) \n",
    "print('Best Kernel:',clf.best_estimator_.kernel)\n",
    "print('Best Gamma:',clf.best_estimator_.gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tells us that the most accurate model uses C=10, the rbf kernel, and gamma=0.001.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check Using Second Dataset\n",
    "\n",
    "Remember the second dataset we created? \n",
    "\n",
    "\n",
    "Now we will use it to prove that those parameters are actually used by the model. \n",
    "\n",
    "\n",
    "First, we apply the classifier we just trained to the second dataset. \n",
    "\n",
    "\n",
    "Then we will train a new support vector classifier from scratch using the parameters found using the grid search. \n",
    "\n",
    "\n",
    "We should get the same results for both models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698870765370138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the classifier trained using data1 to data2, and view the accuracy score\n",
    "clf.score(data2_features, data2_target)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698870765370138"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(data1_features, data1_target).score(data2_features, data2_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV, K-Fold - How do they fit in context of ML Pipeline?\n",
    "\n",
    "### GridSearchCV is a higher-level construct than KFold. The former uses the latter (or others like it).\n",
    "\n",
    "### KFold is a relatively low-level construct that gives you a sequence of train/test indices. \n",
    "\n",
    "### You can use these indices to do several things, including finding the OOB performance of a model, and/or tuning hyperparameters (which basically searches somehow for hyperparameters based on OOB performance).\n",
    "\n",
    "***\n",
    "\n",
    "### GridSearchCV is a higher-level construct, that takes a CV engine like KFold (in its cv argument). \n",
    "\n",
    "### It uses the CV engine to search over hyperparameters (in this case, using grid search over the parameters).\n",
    "\n",
    "****\n",
    "\n",
    "#### Grid Search is used to choose best combination of Hyper parameters of predictive algorithms (Tuning the hyper-parameters of an estimator) whereas KFold Provides train/test indices to split data in train/test sets. It Split dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "#### Each fold is then used once as a validation while the k - 1 remaining folds form the training set. It's used to get better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
